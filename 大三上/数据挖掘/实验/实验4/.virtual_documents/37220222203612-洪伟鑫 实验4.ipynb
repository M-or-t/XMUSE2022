





import pandas as pd
import numpy as np

iris = pd.read_csv('data/iris.csv')  #读取数据文件

data = iris.iloc[:, :4]  #获得样本数据

target = iris.iloc[:,-1].values   #获得样本的类标签


#标准化处理数据
from sklearn.preprocessing import StandardScaler
data_std = StandardScaler().fit_transform(data)



#计算数据集的协方差矩阵
mean_vec = np.mean(data_std,axis = 0)                       #对各列求均值，返回 1* n 矩阵
cov_mat = (data_std - mean_vec).T.dot((data_std - mean_vec))/(data_std.shape[0] - 1) #协方差公式
print('协方差矩阵：\n%s' %cov_mat)





#求解协方差矩阵的特征值及相应的正交化单位特征向量
eig_vals,eig_vecs = np.linalg.eig(cov_mat)
print('特征值:\n%s'%eig_vals)
print('特征向量:\n%s'%eig_vecs)



#将特征值和特征向量对应起来，按照特征值大小排序
eig_pairs = [(np.abs(eig_vals[i]),eig_vecs[:,i])for i in range(len(eig_vals))] 
print('特征值和特征向量：\n',eig_pairs)
print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
eig_pairs.sort(key=lambda x:x[0],reverse=True)
print('特征值由大到小排序结果:')
for i in eig_pairs:
    print(i[0])



#确定主成分的贡献率
total = sum(eig_vals)                                                 #特征值求和
var_exp = [(i/total)*100 for i in sorted(eig_vals,reverse = True)]    #贡献率
print(var_exp)
cum_var_exp = np.cumsum(var_exp)
print(cum_var_exp)                                                    #累积贡献率



#组合第一和第二特征向量实现降维
#保存在data_pca中
matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),
                   eig_pairs[1][1].reshape(4,1)))
data_pca = data_std.dot(matrix_w)
print('PCA降维:\n',data_pca)



#可视化降维后的数据分布
from matplotlib import pyplot as plt
plt.figure(figsize = (6,4))               #二维图展示第一、二主成分
for lab in ('setosa','versicolor','virginica'):
    plt.scatter(data_pca[target == lab,0],data_pca[target == lab,1],label = lab)
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('PCA of Iris Dataset')
plt.legend(loc = 'best')
#plt.tight_layout()
plt.show()



#采用基尼系数作为不纯性度量的决策树分类器
import numpy as np

class DecisionTree:
    def __init__(self):
        pass
    
    def gini(self, y):      #计算基尼系数
        classes = np.unique(y)
        gini = 1
        for cls in classes:
            p_cls = np.mean(y == cls)
            gini -= p_cls ** 2
        return gini
    
    def information_gain(self, X, y, feature_idx, threshold):     #计算信息增益
        parent_gini = self.gini(y)
        left_idxs = X[:, feature_idx] < threshold
        right_idxs = ~left_idxs
        if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:
            return 0
        left_gini = self.gini(y[left_idxs])
        right_gini = self.gini(y[right_idxs])
        child_gini = np.mean(left_idxs) * left_gini + np.mean(right_idxs) * right_gini
        return parent_gini - child_gini
    
    def find_best_split(self, X, y):     #找出最优划分
        best_gain = 0
        best_feature_idx = None
        best_threshold = None
        for feature_idx in range(X.shape[1]):
            thresholds = np.unique(X[:, feature_idx])
            for threshold in thresholds:
                gain = self.information_gain(X, y, feature_idx, threshold)
                if gain > best_gain:
                    best_gain = gain
                    best_feature_idx = feature_idx
                    best_threshold = threshold
        return best_feature_idx, best_threshold
    
    def fit(self, X, y):             #拟合数据
        if len(np.unique(y)) == 1:
            return {'class': y[0]}
        else:
            best_feature_idx, best_threshold = self.find_best_split(X, y)
            left_idxs = X[:, best_feature_idx] < best_threshold
            right_idxs = ~left_idxs
            tree = {
                'feature_idx': best_feature_idx,
                'threshold': best_threshold,
                'left': self.fit(X[left_idxs], y[left_idxs]),
                'right': self.fit(X[right_idxs], y[right_idxs])
            }
            return tree
    
    def predict_sample(self, sample, tree):
        if 'class' in tree:
            return tree['class']
        else:
            if sample[tree['feature_idx']] < tree['threshold']:
                return self.predict_sample(sample, tree['left'])
            else:
                return self.predict_sample(sample, tree['right'])
    
    def predict(self, X, tree):
        return [self.predict_sample(sample, tree) for sample in X]





from sklearn.model_selection import train_test_split


# 将PCA处理过的数据分成训练数据和测试数据,80%用于训练，20%用于测试
X_train, X_test, y_train, y_test = train_test_split(data_pca, target, test_size=0.2, random_state=42)

dt = DecisionTree()
tree = dt.fit(X_train, y_train)   #后续用graphviz对其可视化

#输出训练好的树
print(tree)

# 预测
y_pred = dt.predict(X_test,tree)




from sklearn.metrics import accuracy_score, classification_report

# 评估分类器的性能
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=np.unique(target)))





import os
!pip install graphviz
#用shell执行安装graphviz的命令
from graphviz import Digraph

# 设置Graphviz的安装路径
# 请将路径替换为你的Graphviz安装路径
# 例如：graphviz_path = r'C:\Program Files\Graphviz\bin'
graphviz_path = r'C:\Program Files\Graphviz\bin'

# 将Graphviz的安装路径添加到系统的PATH环境变量中
os.environ["PATH"] += os.pathsep + graphviz_path




#将决策树可视化
import graphviz

def visualize_tree(tree_data, parent=None, node_name=None):
    if node_name is None:
        node_name = str(tree_data['feature_idx']) + ' <= ' + str(tree_data['threshold'])
    if 'class' in tree_data:
        class_label = tree_data['class']
        dot.node(node_name, class_label, shape='box')
    else:
        feature_idx = tree_data['feature_idx']
        threshold = tree_data['threshold']
        dot.node(node_name, f"X[{feature_idx}] <= {threshold}")
        left_child_name = f"{node_name}_left"
        right_child_name = f"{node_name}_right"
        dot.edge(node_name, left_child_name, label="Yes")
        dot.edge(node_name, right_child_name, label="No")
        visualize_tree(tree_data['left'], node_name, left_child_name)
        visualize_tree(tree_data['right'], node_name, right_child_name)

dot = graphviz.Digraph(graph_attr={'size': '8,8'})  # 设置图形大小
visualize_tree(tree)
display(dot)




